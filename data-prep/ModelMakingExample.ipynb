{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "WordVecModelExample.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "LUZz7bdICg-T"
      },
      "outputs": [],
      "source": [
        "import multiprocessing\n",
        "from gensim.models import Word2Vec \n",
        "from gensim.models.phrases import Phrases, Phraser"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Word2VecModelMaker:\n",
        "  # intended to be used with clean corpus (no stopwords or special characters), joined into a single string with full stops left in\n",
        "  def __init__(self, corpus):\n",
        "    self.corpus = corpus\n",
        "    sentences = corpus.split(\".\")\n",
        "    # splitting corpus into sentences and removing very short sentences (won't have useful info on word relations)\n",
        "    self.sentences = [sentence.strip() for sentence in sentences if len(sentence.strip().split(\" \")) > 2]\n",
        "\n",
        "  def make_model(self, vector_length, minimum_count, filename):\n",
        "    cores = multiprocessing.cpu_count()\n",
        "\n",
        "    # process to group together bigrams with distinct meanings\n",
        "    words_list = [i.split() for i in self.sentences]\n",
        "    phrases = Phrases(words_list, min_count=30, progress_per=10000)\n",
        "    bigram = Phraser(phrases)\n",
        "    sents = bigram[words_list]\n",
        "\n",
        "    #setting model properties\n",
        "    wtv_model = Word2Vec(min_count = minimum_count, \n",
        "                     window = 2,\n",
        "                     size = vector_length,\n",
        "                     sample = 6e-5, \n",
        "                     alpha = 0.03, \n",
        "                     min_alpha = 0.0007, \n",
        "                     negative = 20,\n",
        "                     workers = cores - 1)\n",
        "    \n",
        "    # creating and training the model as defined above\n",
        "    wtv_model.build_vocab(sents, progress_per=10000)\n",
        "    wtv_model.train(sents, total_examples=wtv_model.corpus_count, epochs=30, report_delay=1)\n",
        "    # will save the model into current directory. if the model ends up being large there will be multiple files including .npy files\n",
        "    wtv_model.save(filename + \".model\")\n",
        "  \n",
        "  # extremely simplified model to give more differentiated end shapes in frontend\n",
        "  def make_model_simple(self, filename=\"wordvec_simple\"):\n",
        "    self.make_model(vector_length=25, minimum_count=40, filename=filename)\n",
        "\n",
        "  # here example of complex but not fully detailed model\n",
        "  # but the default here intended to keep end json file of shape points relatively small\n",
        "  def make_model_complex(self, filename=\"wordvec_complex\"):\n",
        "    self.make_model(vector_length=150, minimum_count=40, filename=filename)\n",
        "\n",
        "  # standard detailed model, will have multiple model/npy files and end json file will be very large (example run was 500mb)\n",
        "  # mostly for running locally\n",
        "  def make_model_full(self, filename=\"wordvec\"):\n",
        "    self.make_model(vector_length=300, minimum_count=30, filename=filename)"
      ],
      "metadata": {
        "id": "N3kdbLazCdwI"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# example of basic use:\n",
        "\n",
        "# cleaned data loaded in from file, or raw data loaded then cleaned\n",
        "# with open(\"cleaned_text.txt\", \"r\") as file_:\n",
        "#     corpus_cleaned = file_.read()\n",
        "\n",
        "# # make object passing in cleaned corpus\n",
        "# model_maker = Word2VecModelMaker(corpus_cleaned)\n",
        "# # run whichever method fits best, should see the model file in directory in a few minutes depending on size\n",
        "# model_maker.make_model_simple()"
      ],
      "metadata": {
        "id": "lI6PbWBbGvv-"
      },
      "execution_count": 37,
      "outputs": []
    }
  ]
}